{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-15T11:04:10.378833Z","iopub.execute_input":"2023-05-15T11:04:10.379375Z","iopub.status.idle":"2023-05-15T11:04:10.390537Z","shell.execute_reply.started":"2023-05-15T11:04:10.379309Z","shell.execute_reply":"2023-05-15T11:04:10.389723Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/kaggle/input/math-problem-categorization/sample_submission.csv\n/kaggle/input/math-problem-categorization/train.csv\n/kaggle/input/math-problem-categorization/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.516613Z","iopub.execute_input":"2023-05-15T11:04:10.517257Z","iopub.status.idle":"2023-05-15T11:04:10.534378Z","shell.execute_reply.started":"2023-05-15T11:04:10.517222Z","shell.execute_reply":"2023-05-15T11:04:10.533541Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                             problem  category\n0  At a ring toss game at a carnival, Marco throw...        19\n1  What is `471 รท 100`? Interactive: Students fol...        22\n2  Follow the directions below to create an equiv...        20\n3  Solve the following system by substitution:\\n`...        14\n4  Solve the following system by substitution:\\n`...        14","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>problem</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>At a ring toss game at a carnival, Marco throw...</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is `471 รท 100`? Interactive: Students fol...</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Follow the directions below to create an equiv...</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Solve the following system by substitution:\\n`...</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Solve the following system by substitution:\\n`...</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,6))\ndf.groupby('category').problem.count().plot.bar(ylim=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.538369Z","iopub.execute_input":"2023-05-15T11:04:10.538995Z","iopub.status.idle":"2023-05-15T11:04:10.894644Z","shell.execute_reply.started":"2023-05-15T11:04:10.538963Z","shell.execute_reply":"2023-05-15T11:04:10.893573Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"the graph shows that category 7 has more instances in the dataset than other instances. however as the disparencey isnt too much , we will leave it as is for now and focus on futher steps(avg instances of other elements is 10 while that of element 7 is 18 ) ","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import accuracy_score\n\n# # Load the training data\n# train_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# # Split the training data into features and labels\n# X_train = train_data['problem']\n# y_train = train_data['category']\n\n# # Create a count vectorizer to convert text into numerical features\n# vectorizer = CountVectorizer()\n# X_train_vectorized = vectorizer.fit_transform(X_train)\n\n# # Train the classifier\n# classifier = MultinomialNB()\n# classifier.fit(X_train_vectorized, y_train)\n\n# # Load the test data\n# test_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# # Preprocess the test data (if necessary)\n# X_test = test_data['problem']\n\n# # Vectorize the test data\n# X_test_vectorized = vectorizer.transform(X_test)\n\n# # Predict the labels for the test data\n# y_pred = classifier.predict(X_test_vectorized)\n\n# # Print the predicted labels\n# print(y_pred)\n\n# # Evaluate the accuracy (if ground truth labels are available)\n# # ground_truth = pd.read_csv('ground_truth.csv')\n# # y_true = ground_truth['category']\n# # accuracy = accuracy_score(y_true, y_pred)\n# # print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.896063Z","iopub.execute_input":"2023-05-15T11:04:10.899264Z","iopub.status.idle":"2023-05-15T11:04:10.904237Z","shell.execute_reply.started":"2023-05-15T11:04:10.899230Z","shell.execute_reply":"2023-05-15T11:04:10.903262Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import accuracy_score\n\n# # Load the training data\n# train_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# # Split the training data into training and test sets\n# train_data_len = len(train_data)\n# X_train = train_data['problem'][:train_data_len-10]\n# y_train = train_data['category'][:train_data_len-10]\n# X_test = train_data['problem'][train_data_len-10:]\n# y_test_actual = train_data['category'][train_data_len-10:]\n\n# # Create a count vectorizer to convert text into numerical features\n# vectorizer = CountVectorizer()\n# X_train_vectorized = vectorizer.fit_transform(X_train)\n\n# # Train the classifier\n# classifier = MultinomialNB()\n# classifier.fit(X_train_vectorized, y_train)\n\n# # Vectorize the test data\n# X_test_vectorized = vectorizer.transform(X_test)\n\n# # Predict the labels for the test data\n# y_test_pred = classifier.predict(X_test_vectorized)\n\n# # Print the predicted labels\n# print(\"Predicted Labels:\", y_test_pred)\n\n# # Print the actual labels\n# print(\"Actual Labels:\", list(y_test_actual))\n\n# # Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.906141Z","iopub.execute_input":"2023-05-15T11:04:10.906769Z","iopub.status.idle":"2023-05-15T11:04:10.919100Z","shell.execute_reply.started":"2023-05-15T11:04:10.906738Z","shell.execute_reply":"2023-05-15T11:04:10.918103Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.920702Z","iopub.execute_input":"2023-05-15T11:04:10.921488Z","iopub.status.idle":"2023-05-15T11:04:10.938562Z","shell.execute_reply.started":"2023-05-15T11:04:10.921449Z","shell.execute_reply":"2023-05-15T11:04:10.937500Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.940120Z","iopub.execute_input":"2023-05-15T11:04:10.941124Z","iopub.status.idle":"2023-05-15T11:04:10.949955Z","shell.execute_reply.started":"2023-05-15T11:04:10.941086Z","shell.execute_reply":"2023-05-15T11:04:10.948826Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"print(nltk.data.path)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.953048Z","iopub.execute_input":"2023-05-15T11:04:10.954087Z","iopub.status.idle":"2023-05-15T11:04:10.961915Z","shell.execute_reply.started":"2023-05-15T11:04:10.954048Z","shell.execute_reply":"2023-05-15T11:04:10.960729Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n","output_type":"stream"}]},{"cell_type":"code","source":"# !unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/ ","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.963540Z","iopub.execute_input":"2023-05-15T11:04:10.964077Z","iopub.status.idle":"2023-05-15T11:04:10.970017Z","shell.execute_reply.started":"2023-05-15T11:04:10.964037Z","shell.execute_reply":"2023-05-15T11:04:10.968991Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import accuracy_score\n# import string\n# from nltk.corpus import stopwords\n# from nltk.stem import PorterStemmer, WordNetLemmatizer\n# from nltk.tokenize import word_tokenize\n\n# # Load the training data\n# train_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# # Split the training data into training and test sets\n# train_data_len = len(train_data)\n# X_train = train_data['problem'][:train_data_len-10]\n# y_train = train_data['category'][:train_data_len-10]\n# X_test = train_data['problem'][train_data_len-10:]\n# y_test_actual = train_data['category'][train_data_len-10:]\n\n# # Define stopwords, stemmer, and lemmatizer\n# stopwords = set(stopwords.words('english'))\n# stemmer = PorterStemmer()\n# lemmatizer = WordNetLemmatizer()\n\n# # Preprocess the text data\n# def preprocess_text(text):\n#     # Remove punctuation\n#     text = text.translate(str.maketrans('', '', string.punctuation))\n\n#     # Convert text to lowercase\n#     text = text.lower()\n\n#     # Tokenize the text into individual words\n#     tokens = word_tokenize(text)\n\n#     # Remove stop words\n#     tokens = [word for word in tokens if word not in stopwords]\n\n#     # Apply stemming and lemmatization\n#     stemmed_tokens = [stemmer.stem(word) for word in tokens]\n#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n#     # Join the processed tokens back into a single string\n#     processed_text = ' '.join(lemmatized_tokens)\n\n#     return processed_text\n\n# # Preprocess the training data\n# X_train_preprocessed = [preprocess_text(text) for text in X_train]\n\n# # Create a count vectorizer to convert preprocessed text into numerical features\n# vectorizer = CountVectorizer()\n# X_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\n\n# # Train the classifier\n# classifier = MultinomialNB()\n# classifier.fit(X_train_vectorized, y_train)\n\n# # Preprocess the test data\n# X_test_preprocessed = [preprocess_text(text) for text in X_test]\n\n# # Vectorize the preprocessed test data\n# X_test_vectorized = vectorizer.transform(X_test_preprocessed)\n\n# # Predict the labels for the test data\n# y_test_pred = classifier.predict(X_test_vectorized)\n\n# # Print the predicted labels\n# print(\"Predicted Labels:\", y_test_pred)\n\n# # Print the actual labels\n# print(\"Actual Labels:\", list(y_test_actual))\n\n# # Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.972672Z","iopub.execute_input":"2023-05-15T11:04:10.973013Z","iopub.status.idle":"2023-05-15T11:04:10.981943Z","shell.execute_reply.started":"2023-05-15T11:04:10.972981Z","shell.execute_reply":"2023-05-15T11:04:10.981002Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"this step adds some data preprocessing as well.accuracy did drop but i am hoping that is for the best . hope that it reduces ovefitting and overgeneralisation.","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import accuracy_score\n# import string\n# from nltk.corpus import stopwords\n# from nltk.stem import PorterStemmer, WordNetLemmatizer\n# from nltk.tokenize import word_tokenize\n# from scipy.sparse import csr_matrix\n\n# # Load the training data\n# train_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# # Split the training data into training and test sets\n# train_data_len = len(train_data)\n# X_train = train_data['problem'][:train_data_len-10]\n# y_train = train_data['category'][:train_data_len-10]\n# X_test = train_data['problem'][train_data_len-10:]\n# y_test_actual = train_data['category'][train_data_len-10:]\n\n# # Define stopwords, stemmer, and lemmatizer\n# stopwords = set(stopwords.words('english'))\n# stemmer = PorterStemmer()\n# lemmatizer = WordNetLemmatizer()\n\n# # Preprocess the text data\n# def preprocess_text(text):\n#     # Remove punctuation\n#     text = text.translate(str.maketrans('', '', string.punctuation))\n\n#     # Convert text to lowercase\n#     text = text.lower()\n\n#     # Tokenize the text into individual words\n#     tokens = word_tokenize(text)\n\n#     # Remove stop words\n#     tokens = [word for word in tokens if word not in stopwords]\n\n#     # Apply stemming and lemmatization\n#     stemmed_tokens = [stemmer.stem(word) for word in tokens]\n#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n#     # Join the processed tokens back into a single string\n#     processed_text = ' '.join(lemmatized_tokens)\n\n#     return processed_text\n\n# # Preprocess the training data\n# X_train_preprocessed = [preprocess_text(text) for text in X_train]\n\n# # Create a count vectorizer to convert preprocessed text into numerical features\n# vectorizer = CountVectorizer()\n# X_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\n\n# # Extract additional features: length of the problem text\n# X_train_length = [len(text) for text in X_train]\n\n# # Convert the additional features to a sparse matrix\n# X_train_additional_sparse = csr_matrix(X_train_length).transpose()\n\n# # Combine the vectorized text features and additional features\n# X_train_combined = pd.concat([pd.DataFrame(X_train_vectorized.toarray()), pd.DataFrame(X_train_additional_sparse.toarray(), columns=['text_length'])], axis=1)\n\n# # Convert feature names to strings\n# X_train_combined.columns = X_train_combined.columns.astype(str)\n\n# # Train the classifier\n# classifier = MultinomialNB()\n# classifier.fit(X_train_combined, y_train)\n\n# # Preprocess the test data\n# X_test_preprocessed = [preprocess_text(text) for text in X_test]\n\n# # Vectorize the preprocessed test data\n# X_test_vectorized = vectorizer.transform(X_test_preprocessed)\n\n# # Extract additional features: length of the problem text\n# X_test_length = [len(text) for text in X_test]\n\n# # Convert the additional features to a sparse matrix\n# X_test_additional_sparse = csr_matrix(X_test_length).transpose()\n\n# # Combine the vectorized text features and additional features for the test data\n# X_test_combined = pd.concat([pd.DataFrame(X_test_vectorized.toarray()), pd.DataFrame(X_test_length, columns=['text_length'])], axis=1)\n\n# X_test_combined.columns = X_test_combined.columns.astype(str)\n# # Predict the labels for the test data\n# y_test_pred = classifier.predict(X_test_combined)\n\n# # Print the predicted labels\n# print(\"Predicted Labels:\", y_test_pred)\n\n# # Print the actual labels\n# print(\"Actual Labels:\", list(y_test_actual))\n\n# # Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:10.983395Z","iopub.execute_input":"2023-05-15T11:04:10.984027Z","iopub.status.idle":"2023-05-15T11:04:10.997801Z","shell.execute_reply.started":"2023-05-15T11:04:10.983989Z","shell.execute_reply":"2023-05-15T11:04:10.996604Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"so clearly fetaure enginnering did not work well for me so far. the reason for this is that 7 is the most occouring element hence ot dominated the predictins with a wide margin . since feature engineering i s an iterative process, we will come back to it once we have a better model , but will drop this approach for now .","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import accuracy_score\n# from nltk.corpus import stopwords\n# from nltk.stem import PorterStemmer, WordNetLemmatizer\n# from nltk.tokenize import word_tokenize\n# from gensim.models import Word2Vec\n# import numpy as np\n# from sklearn.preprocessing import MinMaxScaler\n\n\n# # Load the training data\n# train_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# # Split the training data into training and test sets\n# train_data_len = len(train_data)\n# X_train = train_data['problem'][:train_data_len-10]\n# y_train = train_data['category'][:train_data_len-10]\n# X_test = train_data['problem'][train_data_len-10:]\n# y_test_actual = train_data['category'][train_data_len-10:]\n\n# # Define stopwords, stemmer, and lemmatizer\n# stopwords = set(stopwords.words('english'))\n# stemmer = PorterStemmer()\n# lemmatizer = WordNetLemmatizer()\n\n# # Preprocess the text data\n# def preprocess_text(text):\n#     # Remove punctuation\n#     text = text.translate(str.maketrans('', '', string.punctuation))\n\n#     # Convert text to lowercase\n#     text = text.lower()\n\n#     # Tokenize the text into individual words\n#     tokens = word_tokenize(text)\n\n#     # Remove stop words\n#     tokens = [word for word in tokens if word not in stopwords]\n\n#     # Apply stemming and lemmatization\n#     stemmed_tokens = [stemmer.stem(word) for word in tokens]\n#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n#     # Join the processed tokens back into a single string\n#     processed_text = ' '.join(lemmatized_tokens)\n\n#     return processed_text\n\n# # Preprocess the training data\n# X_train_preprocessed = [preprocess_text(text) for text in X_train]\n\n# # Train Word2Vec model\n# sentences = [text.split() for text in X_train_preprocessed]\n# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n\n# # Function to transform text into word embeddings\n# def text_to_word_embeddings(text):\n#     word_embeddings = np.zeros((100,), dtype='float32')\n#     word_count = 0\n#     for word in text.split():\n#         if word in word2vec_model.wv.index_to_key:\n#             word_embeddings += word2vec_model.wv.get_vector(word)\n#             word_count += 1\n#     if word_count != 0:\n#         word_embeddings /= word_count\n#     return word_embeddings\n\n\n# # Transform the training data into word embeddings\n# X_train_embeddings = [text_to_word_embeddings(text) for text in X_train_preprocessed]\n\n# # Transform the test data into word embeddings\n# X_test_embeddings = [text_to_word_embeddings(text) for text in X_test]\n\n# # Perform any necessary padding or dimensionality reduction on the embeddings\n\n# # Create a MinMaxScaler\n# scaler = MinMaxScaler()\n\n# # Apply min-max scaling to the word embeddings\n# X_train_scaled = scaler.fit_transform(X_train_embeddings)\n\n# # Train the classifier\n# classifier = MultinomialNB()\n# classifier.fit(X_train_scaled, y_train)\n\n# # Predict the labels for the test data\n# y_test_pred = classifier.predict(X_test_embeddings)\n\n# # Print the predicted labels\n# print(\"Predicted Labels:\", y_test_pred)\n\n# # Print the actual labels\n# print(\"Actual Labels:\", list(y_test_actual))\n\n# # Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:11.000444Z","iopub.execute_input":"2023-05-15T11:04:11.001377Z","iopub.status.idle":"2023-05-15T11:04:11.015984Z","shell.execute_reply.started":"2023-05-15T11:04:11.001335Z","shell.execute_reply":"2023-05-15T11:04:11.015115Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"above code was n attempt at advanced vectorisation techniques to improve performance . however 7 still domiates this problem , so it is about time to hammer down 7 into its place .","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import accuracy_score\n# import string\n# from nltk.corpus import stopwords\n# from nltk.stem import PorterStemmer, WordNetLemmatizer\n# from nltk.tokenize import word_tokenize\n\n# # Load the training data\n# train_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# # Split the training data into training and test sets\n# train_data_len = len(train_data)\n# X_train = train_data['problem'][:train_data_len-10]\n# y_train = train_data['category'][:train_data_len-10]\n# X_test = train_data['problem'][train_data_len-10:]\n# y_test_actual = train_data['category'][train_data_len-10:]\n\n# # Define stopwords, stemmer, and lemmatizer\n# stopwords = set(stopwords.words('english'))\n# stemmer = PorterStemmer()\n# lemmatizer = WordNetLemmatizer()\n\n# # Preprocess the text data\n# def preprocess_text(text):\n#     # Remove punctuation\n#     text = text.translate(str.maketrans('', '', string.punctuation))\n\n#     # Convert text to lowercase\n#     text = text.lower()\n\n#     # Tokenize the text into individual words\n#     tokens = word_tokenize(text)\n\n#     # Remove stop words\n#     tokens = [word for word in tokens if word not in stopwords]\n\n#     # Apply stemming and lemmatization\n#     stemmed_tokens = [stemmer.stem(word) for word in tokens]\n#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n#     # Join the processed tokens back into a single string\n#     processed_text = ' '.join(lemmatized_tokens)\n\n#     return processed_text\n\n# # Preprocess the training data\n# X_train_preprocessed = [preprocess_text(text) for text in X_train]\n\n# # Create a count vectorizer to convert preprocessed text into numerical features\n# vectorizer = CountVectorizer()\n# X_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\n\n# # Train the classifier with class weighting\n# classifier = MultinomialNB(class_prior=None, fit_prior=True)\n# classifier.fit(X_train_vectorized, y_train)\n\n# # Preprocess the test data\n# X_test_preprocessed = [preprocess_text(text) for text in X_test]\n\n# # Vectorize the preprocessed test data\n# X_test_vectorized = vectorizer.transform(X_test_preprocessed)\n\n# # Predict the labels for the test data\n# y_test_pred = classifier.predict(X_test_vectorized)\n\n# # Print the predicted labels\n# print(\"Predicted Labels:\", y_test_pred)\n\n# # Print the actual labels\n# print(\"Actual Labels:\", list(y_test_actual))\n\n# # Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:11.017254Z","iopub.execute_input":"2023-05-15T11:04:11.017796Z","iopub.status.idle":"2023-05-15T11:04:11.031784Z","shell.execute_reply.started":"2023-05-15T11:04:11.017766Z","shell.execute_reply":"2023-05-15T11:04:11.030736Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"results of advanced vectorisation after hammering down 7.","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import accuracy_score\n# import string\n# from nltk.corpus import stopwords\n# from nltk.stem import PorterStemmer, WordNetLemmatizer\n# from nltk.tokenize import word_tokenize\n# from scipy.sparse import csr_matrix\n\n# # Load the training data\n# train_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# # Split the training data into training and test sets\n# train_data_len = len(train_data)\n# X_train = train_data['problem'][:train_data_len-10]\n# y_train = train_data['category'][:train_data_len-10]\n# X_test = train_data['problem'][train_data_len-10:]\n# y_test_actual = train_data['category'][train_data_len-10:]\n\n# # Define stopwords, stemmer, and lemmatizer\n# stopwords = set(stopwords.words('english'))\n# stemmer = PorterStemmer()\n# lemmatizer = WordNetLemmatizer()\n\n# # Preprocess the text data\n# def preprocess_text(text):\n#     # Remove punctuation\n#     text = text.translate(str.maketrans('', '', string.punctuation))\n\n#     # Convert text to lowercase\n#     text = text.lower()\n\n#     # Tokenize the text into individual words\n#     tokens = word_tokenize(text)\n\n#     # Remove stop words\n#     tokens = [word for word in tokens if word not in stopwords]\n\n#     # Apply stemming and lemmatization\n#     stemmed_tokens = [stemmer.stem(word) for word in tokens]\n#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n#     # Join the processed tokens back into a single string\n#     processed_text = ' '.join(lemmatized_tokens)\n\n#     return processed_text\n\n# # Preprocess the training data\n# X_train_preprocessed = [preprocess_text(text) for text in X_train]\n\n# # Create a count vectorizer to convert preprocessed text into numerical features\n# vectorizer = CountVectorizer()\n# X_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\n\n# # Extract additional features: length of the problem text\n# X_train_length = [len(text) for text in X_train]\n\n# # Convert the additional features to a sparse matrix\n# X_train_additional_sparse = csr_matrix(X_train_length).transpose()\n\n# # Combine the vectorized text features and additional features\n# X_train_combined = pd.concat([pd.DataFrame(X_train_vectorized.toarray()), pd.DataFrame(X_train_additional_sparse.toarray(), columns=['text_length'])], axis=1)\n\n# # Convert feature names to strings\n# X_train_combined.columns = X_train_combined.columns.astype(str)\n\n# # Train the classifier with class weighting\n# classifier = MultinomialNB(class_prior=None, fit_prior=True)\n# classifier.fit(X_train_combined, y_train)\n\n# # Preprocess the test data\n# X_test_preprocessed = [preprocess_text(text) for text in X_test]\n\n# # Vectorize the preprocessed test data\n# X_test_vectorized = vectorizer.transform(X_test_preprocessed)\n\n# # Extract additional features: length of the problem text\n# X_test_length = [len(text) for text in X_test]\n\n# # Convert the additional features to a sparse matrix\n# X_test_additional_sparse = csr_matrix(X_test_length).transpose()\n\n# # Combine the vectorized text features and additional features for the test data\n# X_test_combined = pd.concat([pd.DataFrame(X_test_vectorized.toarray()), pd.DataFrame(X_test_length, columns=['text_length'])], axis=1)\n\n# X_test_combined.columns = X_test_combined.columns.astype(str)\n# # Predict the labels for the test data\n# y_test_pred = classifier.predict(X_test_combined)\n\n# # Print the predicted labels\n# print(\"Predicted Labels:\", y_test_pred)\n\n# # Print the actual labels\n# print(\"Actual Labels:\", list(y_test_actual))\n\n# # Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:11.033313Z","iopub.execute_input":"2023-05-15T11:04:11.033915Z","iopub.status.idle":"2023-05-15T11:04:11.045168Z","shell.execute_reply.started":"2023-05-15T11:04:11.033885Z","shell.execute_reply":"2023-05-15T11:04:11.044196Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"this indicates that still the weigh of 7 is too high , even with dumming down of it . we will try different approach later.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# Load the test data\ntest_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# Combine training and test data for preprocessing\nall_data = pd.concat([train_data, test_data])\n\n# Define stopwords, stemmer, and lemmatizer\nstopwords = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess the text data\ndef preprocess_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Tokenize the text into individual words\n    tokens = word_tokenize(text)\n\n    # Remove stop words\n    tokens = [word for word in tokens if word not in stopwords]\n\n    # Apply stemming and lemmatization\n    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n    # Join the processed tokens back into a single string\n    processed_text = ' '.join(lemmatized_tokens)\n\n    return processed_text\n\n# Preprocess the data\nX_all = all_data['problem']\nX_all_preprocessed = [preprocess_text(text) for text in X_all]\n\n# Create a count vectorizer with n-gram range (1, 2)\nvectorizer = CountVectorizer(ngram_range=(1, 2))\nX_all_vectorized = vectorizer.fit_transform(X_all_preprocessed)\n\n# Split the data back into training and test sets\nX_train_vectorized = X_all_vectorized[:len(train_data)]\nX_test_vectorized = X_all_vectorized[len(train_data):]\n\ny_train = train_data['category']\ny_test_actual = test_data['category']\n\n# Train the classifier with class weighting\nclassifier = MultinomialNB(class_prior=None, fit_prior=True)\nclassifier.fit(X_train_vectorized, y_train)\n\n# Predict the labels for the test data\ny_test_pred = classifier.predict(X_test_vectorized)\n\n# Print the predicted labels\nprint(\"Predicted Labels:\", y_test_pred)\n\n\n# Print the actual labels\n# print(\"Actual Labels:\", list(y_test_actual))\n\n# Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:32.593090Z","iopub.execute_input":"2023-05-15T11:04:32.593536Z","iopub.status.idle":"2023-05-15T11:04:32.891764Z","shell.execute_reply.started":"2023-05-15T11:04:32.593502Z","shell.execute_reply":"2023-05-15T11:04:32.890603Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Predicted Labels: [17  2 14 10  6 12  5  2  6 20 13 15  7 13  0  2 12 20 20  7]\nActual Labels: [7, 4, 14, 10, 6, 12, 4, 4, 6, 7, 13, 15, 7, 13, 0, 4, 12, 20, 20, 7]\nAccuracy: 0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# Load the test data\ntest_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# Combine training and test data for preprocessing\nall_data = pd.concat([train_data, test_data])\n\n# Define stopwords, stemmer, and lemmatizer\nstopwords = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess the text data\ndef preprocess_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Tokenize the text into individual words\n    tokens = word_tokenize(text)\n\n    # Remove stop words\n    tokens = [word for word in tokens if word not in stopwords]\n\n    # Apply stemming and lemmatization\n    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n    # Join the processed tokens back into a single string\n    processed_text = ' '.join(lemmatized_tokens)\n\n    return processed_text\n\n# Preprocess the data\nX_all = all_data['problem']\nX_all_preprocessed = [preprocess_text(text) for text in X_all]\n\n# Create a count vectorizer with n-gram range (1, 2)\nvectorizer = CountVectorizer(ngram_range=(1, 2))\nX_all_vectorized = vectorizer.fit_transform(X_all_preprocessed)\n\n# Split the data back into training and test sets\nX_train_vectorized = X_all_vectorized[:len(train_data)]\nX_test_vectorized = X_all_vectorized[len(train_data):]\n\ny_train = train_data['category']\ny_test_actual = test_data['category']\n\n# Train the classifier with class weighting\nclassifier = MultinomialNB(class_prior=None, fit_prior=True)\nclassifier.fit(X_train_vectorized, y_train)\n\n# Predict the labels for the test data\ny_test_pred = classifier.predict(X_test_vectorized)\n\n# Print the predicted labels\nprint(\"Predicted Labels:\", y_test_pred)\n\n\n# Print the actual labels\n# print(\"Actual Labels:\", list(y_test_actual))\n\n# Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:05:19.832039Z","iopub.execute_input":"2023-05-15T11:05:19.832433Z","iopub.status.idle":"2023-05-15T11:05:20.122786Z","shell.execute_reply.started":"2023-05-15T11:05:19.832402Z","shell.execute_reply":"2023-05-15T11:05:20.121680Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Predicted Labels: [17  8  7  7  5 23  3  3 23  8  1  3  3  7 11 23  2  2  2 16 16 22  7 19\n 10  9 18 12 13 16 14  8 19 22 11  7 17  8 18  1 17 19 12  3 12 20  0  2\n  0  8 23 11 23 23 12 17 20  4 12 18 21 16  9 13  2  6 14 19  8  5  1 22\n  7 13  7  5  0 14  1 11  7 12 16 23 11 15  6 19  9 12  8 22 23  3 14 11\n 20  1 23 12  6 19 14  5  3 20  5  6  3 12  0  0  9 10 12 20 23 10 16  9\n  8  6 20 22  2]\n","output_type":"stream"}]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'problem': test_data['problem'], 'category': y_test_pred})\nmy_submission.to_csv('submission2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:04:13.110890Z","iopub.execute_input":"2023-05-15T11:04:13.111236Z","iopub.status.idle":"2023-05-15T11:04:13.124181Z","shell.execute_reply.started":"2023-05-15T11:04:13.111199Z","shell.execute_reply":"2023-05-15T11:04:13.123016Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"this one ngram +preprocessing+weighted classification is useful .but we need better . much much better.try more.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# Load the test data\ntest_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# Combine training and test data for preprocessing\nall_data = pd.concat([train_data, test_data])\n\n# Define stopwords, stemmer, and lemmatizer\nstopwords = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess the text data\ndef preprocess_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Tokenize the text into individual words\n    tokens = word_tokenize(text)\n\n    # Remove stop words\n    tokens = [word for word in tokens if word not in stopwords]\n\n    # Apply stemming and lemmatization\n    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n    # Join the processed tokens back into a single string\n    processed_text = ' '.join(lemmatized_tokens)\n\n    return processed_text\n\n# Preprocess the data\nX_all = all_data['problem']\nX_all_preprocessed = [preprocess_text(text) for text in X_all]\n\n# Create a count vectorizer with n-gram range (1, 2)\nvectorizer = CountVectorizer(ngram_range=(1, 2))\nX_all_vectorized = vectorizer.fit_transform(X_all_preprocessed)\n\n# Split the data back into training and test sets\nX_train_vectorized = X_all_vectorized[:len(train_data)]\nX_test_vectorized = X_all_vectorized[len(train_data):]\n\ny_train = train_data['category']\ny_test_actual = test_data['category']\n\n# Define individual classifiers\nclassifier1 = MultinomialNB(class_prior=None, fit_prior=True)\nclassifier2 = MultinomialNB(class_prior=None, fit_prior=False)\n\n# Create the Voting Classifier\nensemble_classifier = VotingClassifier(\n    estimators=[('nb1', classifier1), ('nb2', classifier2)],\n    voting='hard'\n)\n\n# Train the ensemble classifier\nensemble_classifier.fit(X_train_vectorized, y_train)\n\n# Predict the labels for the test data\ny_test_pred = ensemble_classifier.predict(X_test_vectorized)\n\n# Print the predicted labels\nprint(\"Predicted Labels:\", y_test_pred)\n\n# Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:17:32.018123Z","iopub.execute_input":"2023-05-15T11:17:32.018530Z","iopub.status.idle":"2023-05-15T11:17:32.329718Z","shell.execute_reply.started":"2023-05-15T11:17:32.018497Z","shell.execute_reply":"2023-05-15T11:17:32.328825Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Predicted Labels: [17  8  7  7  5 23  3  3 23  8  1  3  3  7 11 23  2  2  2 16 16 22  7 19\n 10  9 18 12 13 16 14  8 19 22 11  7 17  8 18  1 17 19 12  3 12 20  0  2\n  0  8 23 11 23 23 12 17 20  4 12 18 21 16  9 13  2  6 14 19  8  5  1 22\n  7 13  7  5  0 14  1 11  7 12 16 23 11 15  6 19  9 12  8 22 23  3 14 11\n 20  1 23 12  6 19 14  5  3 20  5  6  3 12  0  0  9 10 12 20 23 10 16  9\n  8  6 20 22  2]\n","output_type":"stream"}]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'problem': test_data['problem'], 'category': y_test_pred})\nmy_submission.to_csv('submission3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:17:35.558183Z","iopub.execute_input":"2023-05-15T11:17:35.558566Z","iopub.status.idle":"2023-05-15T11:17:35.566629Z","shell.execute_reply.started":"2023-05-15T11:17:35.558536Z","shell.execute_reply":"2023-05-15T11:17:35.565560Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"ensemble classifier is not improving the performance. if anything , it isnt reducing either , so thats nice . i will try some advanced ensemble methods to improve the performance .","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# Load the test data\ntest_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# Combine training and test data for preprocessing\nall_data = pd.concat([train_data, test_data])\n\n# Define stopwords, stemmer, and lemmatizer\nstopwords = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess the text data\ndef preprocess_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Tokenize the text into individual words\n    tokens = word_tokenize(text)\n\n    # Remove stop words\n    tokens = [word for word in tokens if word not in stopwords]\n\n    # Apply stemming and lemmatization\n    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n    # Join the processed tokens back into a single string\n    processed_text = ' '.join(lemmatized_tokens)\n\n    return processed_text\n\n# Preprocess the data\nX_all = all_data['problem']\nX_all_preprocessed = [preprocess_text(text) for text in X_all]\n\n# Create a count vectorizer with n-gram range (1, 2)\nvectorizer = CountVectorizer(ngram_range=(1, 2))\nX_all_vectorized = vectorizer.fit_transform(X_all_preprocessed)\n\n# Split the data back into training and test sets\nX_train_vectorized = X_all_vectorized[:220]\nX_test_vectorized = X_all_vectorized[220:240]\n\ny_train = train_data['category'][:220]\ny_test_actual = train_data['category'][220:240]\n\n# Train the classifier\nclassifier = RandomForestClassifier(n_estimators=100)\nclassifier.fit(X_train_vectorized, y_train)\n\n# Predict the labels for the test data\ny_test_pred = classifier.predict(X_test_vectorized)\n\n# Print the predicted labels\nprint(\"Predicted Labels:\", y_test_pred)\n\n# Evaluate the accuracy\naccuracy = accuracy_score(y_test_actual, y_test_pred)\nprint('Accuracy:', accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:30:48.393886Z","iopub.execute_input":"2023-05-15T11:30:48.394284Z","iopub.status.idle":"2023-05-15T11:30:49.055631Z","shell.execute_reply.started":"2023-05-15T11:30:48.394250Z","shell.execute_reply":"2023-05-15T11:30:49.054413Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Predicted Labels: [17  1 14 10  6 12  1  1  6 21 13 15  7 14  0  1 12 20 20  7]\nAccuracy: 0.65\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# Load the test data\ntest_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# Combine training and test data for preprocessing\nall_data = pd.concat([train_data, test_data])\n\n# Define stopwords, stemmer, and lemmatizer\nstopwords = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess the text data\ndef preprocess_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Tokenize the text into individual words\n    tokens = word_tokenize(text)\n\n    # Remove stop words\n    tokens = [word for word in tokens if word not in stopwords]\n\n    # Apply stemming and lemmatization\n    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n    # Join the processed tokens back into a single string\n    processed_text = ' '.join(lemmatized_tokens)\n\n    return processed_text\n\n# Preprocess the data\nX_train = train_data['problem']\nX_train_preprocessed = [preprocess_text(text) for text in X_train]\n\nX_test = test_data['problem']\nX_test_preprocessed = [preprocess_text(text) for text in X_test]\n\n# Create a count vectorizer with n-gram range (1, 2)\nvectorizer = CountVectorizer(ngram_range=(1, 2))\nX_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\nX_test_vectorized = vectorizer.transform(X_test_preprocessed)\n\ny_train = train_data['category']\ny_test_actual = test_data['category']\n\n# Train the classifier\nclassifier = RandomForestClassifier(n_estimators=100)\nclassifier.fit(X_train_vectorized, y_train)\n\n# Predict the labels for the test data\ny_test_pred = classifier.predict(X_test_vectorized)\n\n# Print the predicted labels\nprint(\"Predicted Labels:\", y_test_pred)\n\n# Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:30:51.767019Z","iopub.execute_input":"2023-05-15T11:30:51.767449Z","iopub.status.idle":"2023-05-15T11:30:52.371222Z","shell.execute_reply.started":"2023-05-15T11:30:51.767414Z","shell.execute_reply":"2023-05-15T11:30:52.369416Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Predicted Labels: [21  8  8  7  4 21  3  1 21  9  1  1 23  9 12 21  1  1  1 16 16 22  9 17\n 21  9 16 12 12 18 12 21 21 22 12 21 21  8 18  1 21 17 21  3 12 20  0  2\n  0  8 21 14 21 21 21 17 20  1 12 16 21 18  9 12  1  6 12 17 18  1  1 22\n  8 12  8 24  0 12  1 12  9 21 16 21 12 21  6 17  9 12  8 22 21  3 12 12\n 20  1 21 21  6 17 12  1  3 20  1  6  3 12  0  0  9  7 12 21 21 10 18  9\n  8  6 20 22  1]\n","output_type":"stream"}]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'problem': test_data['problem'], 'category': y_test_pred})\nmy_submission.to_csv('submission4.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:30:58.797359Z","iopub.execute_input":"2023-05-15T11:30:58.797893Z","iopub.status.idle":"2023-05-15T11:30:58.807830Z","shell.execute_reply.started":"2023-05-15T11:30:58.797853Z","shell.execute_reply":"2023-05-15T11:30:58.806396Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"that was not an improvement . if anthing it brought us baack to where we started . better to take the old model and use grid search etc.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# Load the test data\ntest_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# Combine training and test data for preprocessing\nall_data = pd.concat([train_data, test_data])\n\n# Define stopwords, stemmer, and lemmatizer\nstopwords = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess the text data\ndef preprocess_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Tokenize the text into individual words\n    tokens = word_tokenize(text)\n\n    # Remove stop words\n    tokens = [word for word in tokens if word not in stopwords]\n\n    # Apply stemming and lemmatization\n    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n    # Join the processed tokens back into a single string\n    processed_text = ' '.join(lemmatized_tokens)\n\n    return processed_text\n\n# Preprocess the data\nX_train = train_data['problem']\nX_train_preprocessed = [preprocess_text(text) for text in X_train]\n\nX_test = test_data['problem']\nX_test_preprocessed = [preprocess_text(text) for text in X_test]\n\n# Create a count vectorizer with n-gram range (1, 2)\nvectorizer = CountVectorizer(ngram_range=(1, 2))\nX_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\nX_test_vectorized = vectorizer.transform(X_test_preprocessed)\n\ny_train = train_data['category']\ny_test_actual = test_data['category']\n\n# Define the parameter grid for grid search\nparam_grid = {'alpha': [0.1, 0.5, 1.0], 'fit_prior': [True, False]}\n# Perform grid search to find the best hyperparameters\ngrid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5)\ngrid_search.fit(X_train_vectorized, y_train)\n\n# Get the best model from grid search\nbest_model = grid_search.best_estimator_\n\n# Predict the labels for the test data\ny_test_pred = best_model.predict(X_test_vectorized)\n\n# Print the predicted labels\nprint(\"Predicted Labels:\", y_test_pred)\n\n# Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:53:59.037387Z","iopub.execute_input":"2023-05-15T11:53:59.037807Z","iopub.status.idle":"2023-05-15T11:53:59.491952Z","shell.execute_reply.started":"2023-05-15T11:53:59.037776Z","shell.execute_reply":"2023-05-15T11:53:59.490165Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Predicted Labels: [17  7  7  7  5  7  3  3  7  8  1  3  3  7 11  7  2  2  2 16 16 22  7 19\n 10  9 18 12 13 16 14  8 19 22 11  7 17  9 18  1 17 19 12  3 12 20  0  2\n  0  7  7 11  7  7 12 17 20  4 12 18 21 16  9 13  2  6 14 19  7  5  1 22\n  7 13  7  5  0 14  1 11  7 12 16  7 11 15  6 19  9 12  8 22  7  3 14 11\n 20  1  7 12  6 19 14  5  3 20  5  6  3 12  0  0  7  7 12 20  7  7 16  9\n  7  6 20 22  2]\n","output_type":"stream"}]},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import accuracy_score\n# import string\n# from nltk.corpus import stopwords\n# from nltk.stem import PorterStemmer, WordNetLemmatizer\n# from nltk.tokenize import word_tokenize\n\n# # Load the training data\n# train_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# # Load the test data\n# test_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# # Combine training and test data for preprocessing\n# all_data = pd.concat([train_data, test_data])\n\n# # Define stopwords, stemmer, and lemmatizer\n# stopwords = set(stopwords.words('english'))\n# stemmer = PorterStemmer()\n# lemmatizer = WordNetLemmatizer()\n\n# # Preprocess the text data\n# def preprocess_text(text):\n#     # Remove punctuation\n#     text = text.translate(str.maketrans('', '', string.punctuation))\n\n#     # Convert text to lowercase\n#     text = text.lower()\n\n#     # Tokenize the text into individual words\n#     tokens = word_tokenize(text)\n\n#     # Remove stop words\n#     tokens = [word for word in tokens if word not in stopwords]\n\n#     # Apply stemming and lemmatization\n#     stemmed_tokens = [stemmer.stem(word) for word in tokens]\n#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n#     # Join the processed tokens back into a single string\n#     processed_text = ' '.join(lemmatized_tokens)\n\n#     return processed_text\n\n# # Preprocess the data\n# X_train = train_data['problem'][:220]\n# X_train_preprocessed = [preprocess_text(text) for text in X_train]\n\n# X_test = train_data['problem'][220:240]\n# X_test_preprocessed = [preprocess_text(text) for text in X_test]\n\n# # Create a count vectorizer with n-gram range (1, 2)\n# vectorizer = CountVectorizer(ngram_range=(1, 2))\n# X_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\n# X_test_vectorized = vectorizer.transform(X_test_preprocessed)\n\n# y_train = train_data['category'][:220]\n# y_test_actual = train_data['category'][220:240]\n\n# # Define the parameter grid for grid search\n# param_grid = {'alpha': [0.1, 0.5, 1.0], 'fit_prior': [True, False]}\n\n# # Perform grid search to find the best hyperparameters\n# grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5)\n# grid_search.fit(X_train_vectorized, y_train)\n\n# # Get the best model from grid search\n# best_model = grid_search.best_estimator_\n\n# # Predict the labels for the test data\n# y_test_pred = best_model.predict(X_test_vectorized)\n\n# # Print the predicted labels\n# print(\"Predicted Labels:\", y_test_pred)\n\n# # Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:52:15.845671Z","iopub.execute_input":"2023-05-15T11:52:15.846369Z","iopub.status.idle":"2023-05-15T11:52:16.174461Z","shell.execute_reply.started":"2023-05-15T11:52:15.846302Z","shell.execute_reply":"2023-05-15T11:52:16.173291Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Predicted Labels: [17  2 14 10  6 12  5  2  6 20 13 15  7 13  0  2 12 20 20  7]\nAccuracy: 0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'problem': test_data['problem'], 'category': y_test_pred})\nmy_submission.to_csv('submission5_1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T11:54:18.148780Z","iopub.execute_input":"2023-05-15T11:54:18.149202Z","iopub.status.idle":"2023-05-15T11:54:18.157386Z","shell.execute_reply.started":"2023-05-15T11:54:18.149169Z","shell.execute_reply":"2023-05-15T11:54:18.156312Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"added grid search but that only worsened things . now acc down to 53 %. need another solution asap.\nfit_prior did also not help.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# Load the test data\ntest_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# Combine training and test data for preprocessing\nall_data = pd.concat([train_data, test_data])\n\n# Define stopwords, stemmer, and lemmatizer\nstopwords = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess the text data\ndef preprocess_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Tokenize the text into individual words\n    tokens = word_tokenize(text)\n\n    # Remove stop words\n    tokens = [word for word in tokens if word not in stopwords]\n\n    # Apply stemming and lemmatization\n    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n    # Join the processed tokens back into a single string\n    processed_text = ' '.join(lemmatized_tokens)\n\n    return processed_text\n\n# Preprocess the data\nX_train = train_data['problem'][:220]\nX_train_preprocessed = [preprocess_text(text) for text in X_train]\n\nX_test = train_data['problem'][220:240]\nX_test_preprocessed = [preprocess_text(text) for text in X_test]\n\n# Create a TF-IDF vectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2))\nX_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\nX_test_vectorized = vectorizer.transform(X_test_preprocessed)\n\ny_train = train_data['category'][:220]\ny_test_actual = train_data['category'][220:240]\n\n# Define the parameter grid for grid search\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n}\n\n# Perform grid search to find the best hyperparameters\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\ngrid_search.fit(X_train_vectorized, y_train)\n\n# Get the best model from grid search\nbest_model = grid_search.best_estimator_\n\n# Predict the labels for the test data\ny_test_pred = best_model.predict(X_test_vectorized)\n\n# Print the predicted labels\nprint(\"Predicted Labels:\", y_test_pred)\n\n# Evaluate the accuracy\naccuracy = accuracy_score(y_test_actual, y_test_pred)\nprint('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:12.331299Z","iopub.execute_input":"2023-05-15T12:01:12.331778Z","iopub.status.idle":"2023-05-15T12:04:18.381353Z","shell.execute_reply.started":"2023-05-15T12:01:12.331743Z","shell.execute_reply":"2023-05-15T12:04:18.378619Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Predicted Labels: [17  1 14 10  6 12  1  1  6 21 13 15  7 13  0  1 12 20 20  7]\nAccuracy: 0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\n\n# Load the test data\ntest_data = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\n\n# Combine training and test data for preprocessing\nall_data = pd.concat([train_data, test_data])\n\n# Define stopwords, stemmer, and lemmatizer\nstopwords = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess the text data\ndef preprocess_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Tokenize the text into individual words\n    tokens = word_tokenize(text)\n\n    # Remove stop words\n    tokens = [word for word in tokens if word not in stopwords]\n\n    # Apply stemming and lemmatization\n    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n\n    # Join the processed tokens back into a single string\n    processed_text = ' '.join(lemmatized_tokens)\n\n    return processed_text\n\n# Preprocess the data\nX_train = train_data['problem']\nX_train_preprocessed = [preprocess_text(text) for text in X_train]\n\nX_test = test_data['problem']\nX_test_preprocessed = [preprocess_text(text) for text in X_test]\n\n# Create a TF-IDF vectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2))\nX_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\nX_test_vectorized = vectorizer.transform(X_test_preprocessed)\n\ny_train = train_data['category']\ny_test_actual = test_data['category']\n\n# Define the parameter grid for grid search\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n}\n\n# Perform grid search to find the best hyperparameters\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\ngrid_search.fit(X_train_vectorized, y_train)\n\n# Get the best model from grid search\nbest_model = grid_search.best_estimator_\n\n# Predict the labels for the test data\ny_test_pred = best_model.predict(X_test_vectorized)\n\n# Print the predicted labels\nprint(\"Predicted Labels:\", y_test_pred)\n\n# Evaluate the accuracy\n# accuracy = accuracy_score(y_test_actual, y_test_pred)\n# print('Accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:07:08.859174Z","iopub.execute_input":"2023-05-15T12:07:08.860043Z","iopub.status.idle":"2023-05-15T12:10:16.445115Z","shell.execute_reply.started":"2023-05-15T12:07:08.859993Z","shell.execute_reply":"2023-05-15T12:10:16.443514Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Predicted Labels: [19  8  4  7  1 23  3  1 23  9  1  1  1  7 11 23  4  4  1 16 16 22  9 17\n 10  9 16 14 14 18 14  7 21 22 11  7 19  8 18  1 19 17 12  3 12 22  0  4\n  0  8 23 11 23 23 12 17 22  1 14 16  7 18  9 13  1  6 14 17 18  1  1 22\n  8 14  8  1  0 14  1 14  9 12 16 23 11 15  6 17  9 12  4 22 23  3 14 11\n 22  1 23 15  6 17 14  1  3 22  1  6  3 12  0  0  9  7 14 21 23  9 19  9\n  1  6 22 22  1]\n","output_type":"stream"}]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'problem': test_data['problem'], 'category': y_test_pred})\nmy_submission.to_csv('submission5_2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:10:50.925815Z","iopub.execute_input":"2023-05-15T12:10:50.926262Z","iopub.status.idle":"2023-05-15T12:10:50.934828Z","shell.execute_reply.started":"2023-05-15T12:10:50.926213Z","shell.execute_reply":"2023-05-15T12:10:50.933135Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"replaced the MultinomialNB classifier with a RandomForestClassifier. The parameter grid for grid search includes the number of estimators, maximum depth, minimum samples split, and minimum samples leaf. The grid search will search for the best combination of these parameters to maximize accuracy. ","metadata":{}}]}